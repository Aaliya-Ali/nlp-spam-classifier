# Import necessary libraries
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load Fashion MNIST dataset
mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)
x = mnist.data.astype('float32') / 255.0
y = mnist.target.astype('int')

# One-hot encode the labels
ohe = OneHotEncoder(sparse_output=False)
y_ohe = ohe.fit_transform(y.reshape(-1, 1))

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y_ohe, test_size=0.2, random_state=42)

# Define activation functions and their derivatives
def relu(z): return np.maximum(0, z)
def relu_deriv(z): return (z > 0).astype(float)
def softmax(z):
    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return e_z / np.sum(e_z, axis=1, keepdims=True)
def cross_entropy(y_true, y_pred):
    return -np.sum(y_true * np.log(y_pred + 1e-8)) / y_true.shape[0]

def initialize_parameters(layer_dims):
    np.random.seed(42)
    parameters = {}
    for l in range(1, len(layer_dims)):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01
        parameters['b' + str(l)] = np.zeros((1, layer_dims[l]))
    return parameters

def forward_propagation(X, parameters):
    cache = {'A0': X}
    L = len(parameters) // 2
    for l in range(1, L):
        Z = np.dot(cache['A'+str(l-1)], parameters['W'+str(l)]) + parameters['b'+str(l)]
        cache['Z'+str(l)] = Z
        cache['A'+str(l)] = relu(Z)
    ZL = np.dot(cache['A'+str(L-1)], parameters['W'+str(L)]) + parameters['b'+str(L)]
    cache['Z'+str(L)] = ZL
    cache['A'+str(L)] = softmax(ZL)
    return cache

def backward_propagation(X, Y, parameters, cache):
    grads = {}
    L = len(parameters) // 2
    m = X.shape[0]
    dZL = cache['A'+str(L)] - Y
    grads['dW'+str(L)] = np.dot(cache['A'+str(L-1)].T, dZL) / m
    grads['db'+str(L)] = np.sum(dZL, axis=0, keepdims=True) / m
    for l in reversed(range(1, L)):
        dZ = np.dot(dZL, parameters['W'+str(l+1)].T) * relu_deriv(cache['Z'+str(l)])
        grads['dW'+str(l)] = np.dot(cache['A'+str(l-1)].T, dZ) / m
        grads['db'+str(l)] = np.sum(dZ, axis=0, keepdims=True) / m
        dZL = dZ
    return grads

def sgd(parameters, grads, learning_rate):
    for l in range(1, len(parameters) // 2 + 1):
        parameters['W'+str(l)] -= learning_rate * grads['dW'+str(l)]
        parameters['b'+str(l)] -= learning_rate * grads['db'+str(l)]

def msgd(parameters, grads, v, learning_rate, beta=0.9):
    for l in range(1, len(parameters) // 2 + 1):
        v['dW'+str(l)] = beta * v['dW'+str(l)] + (1 - beta) * grads['dW'+str(l)]
        v['db'+str(l)] = beta * v['db'+str(l)] + (1 - beta) * grads['db'+str(l)]
        parameters['W'+str(l)] -= learning_rate * v['dW'+str(l)]
        parameters['b'+str(l)] -= learning_rate * v['db'+str(l)]

def rmsprop(parameters, grads, s, learning_rate, beta=0.9, epsilon=1e-8):
    for l in range(1, len(parameters) // 2 + 1):
        s['dW'+str(l)] = beta * s['dW'+str(l)] + (1 - beta) * grads['dW'+str(l)]**2
        s['db'+str(l)] = beta * s['db'+str(l)] + (1 - beta) * grads['db'+str(l)]**2
        parameters['W'+str(l)] -= learning_rate * grads['dW'+str(l)] / (np.sqrt(s['dW'+str(l)]) + epsilon)
        parameters['b'+str(l)] -= learning_rate * grads['db'+str(l)] / (np.sqrt(s['db'+str(l)]) + epsilon)

def adam(parameters, grads, m, v, t, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):
    for l in range(1, len(parameters) // 2 + 1):
        m['dW'+str(l)] = beta1 * m['dW'+str(l)] + (1 - beta1) * grads['dW'+str(l)]
        m['db'+str(l)] = beta1 * m['db'+str(l)] + (1 - beta1) * grads['db'+str(l)]
        v['dW'+str(l)] = beta2 * v['dW'+str(l)] + (1 - beta2) * (grads['dW'+str(l)]**2)
        v['db'+str(l)] = beta2 * v['db'+str(l)] + (1 - beta2) * (grads['db'+str(l)]**2)
        m_hat_dw = m['dW'+str(l)] / (1 - beta1**t)
        m_hat_db = m['db'+str(l)] / (1 - beta1**t)
        v_hat_dw = v['dW'+str(l)] / (1 - beta2**t)
        v_hat_db = v['db'+str(l)] / (1 - beta2**t)
        parameters['W'+str(l)] -= learning_rate * m_hat_dw / (np.sqrt(v_hat_dw) + epsilon)
        parameters['b'+str(l)] -= learning_rate * m_hat_db / (np.sqrt(v_hat_db) + epsilon)

def train(X, Y, X_val, Y_val, layer_dims, optimizer='sgd', learning_rate=0.01, epochs=10):
    parameters = initialize_parameters(layer_dims)
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    m, v, s = {}, {}, {}
    for l in range(1, len(layer_dims)):
        m['dW'+str(l)] = np.zeros_like(parameters['W'+str(l)])
        m['db'+str(l)] = np.zeros_like(parameters['b'+str(l)])
        v['dW'+str(l)] = np.zeros_like(parameters['W'+str(l)])
        v['db'+str(l)] = np.zeros_like(parameters['b'+str(l)])
        s['dW'+str(l)] = np.zeros_like(parameters['W'+str(l)])
        s['db'+str(l)] = np.zeros_like(parameters['b'+str(l)])

    for epoch in range(1, epochs+1):
        cache = forward_propagation(X, parameters)
        grads = backward_propagation(X, Y, parameters, cache)

        if optimizer == 'sgd':
            sgd(parameters, grads, learning_rate)
        elif optimizer == 'msgd':
            msgd(parameters, grads, m, learning_rate)
        elif optimizer == 'rmsprop':
            rmsprop(parameters, grads, s, learning_rate)
        elif optimizer == 'adam':
            adam(parameters, grads, m, v, epoch, learning_rate)

        y_pred = np.argmax(cache['A'+str(len(layer_dims)-1)], axis=1)
        y_true = np.argmax(Y, axis=1)
        train_acc = accuracy_score(y_true, y_pred)
        train_loss = cross_entropy(Y, cache['A'+str(len(layer_dims)-1)])

        val_cache = forward_propagation(X_val, parameters)
        val_pred = np.argmax(val_cache['A'+str(len(layer_dims)-1)], axis=1)
        val_true = np.argmax(Y_val, axis=1)
        val_acc = accuracy_score(val_true, val_pred)
        val_loss = cross_entropy(Y_val, val_cache['A'+str(len(layer_dims)-1)])

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        print(f"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")
    return parameters, history

# Example: train with Adam optimizer
params, history = train(x_train, y_train, x_test, y_test, [784, 128, 64, 10], optimizer='adam', learning_rate=0.001, epochs=10)
